#!/usr/bin/env python3
"""
Weibo Trends Analyzer - Main Script
Analyzes Weibo trending topics and generates creative product ideas
"""
import os
import sys
import asyncio
import json
from datetime import datetime
from typing import Dict, List
from jinja2 import Environment, FileSystemLoader

# Add parent directory to path to import utils
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

from scripts.utils import (
    WeiboAPIClient,
    SearchAPIClient,
    format_timestamp,
    format_display_timestamp,
    validate_product_concept,
    calculate_score_tier,
    save_json_data
)

# Import Claude Agent SDK
try:
    from claude_agent_sdk import query
except ImportError:
    print("âŒ Error: claude-agent-sdk not installed. Please run: pip install claude-agent-sdk")
    sys.exit(1)


class WeiboTrendsAnalyzer:
    """Main analyzer class"""

    def __init__(
        self,
        tianapi_key: str,
        search_api_key: str,
        anthropic_api_key: str,
        search_engine: str = "serpapi",
        anthropic_base_url: str = None
    ):
        self.weibo_client = WeiboAPIClient(tianapi_key)
        self.search_client = SearchAPIClient(search_api_key, search_engine)
        self.anthropic_api_key = anthropic_api_key
        self.anthropic_base_url = anthropic_base_url

        # Set environment variables for Claude SDK
        os.environ["ANTHROPIC_API_KEY"] = anthropic_api_key

        # Set custom API base URL if provided (for third-party APIs)
        if anthropic_base_url:
            os.environ["ANTHROPIC_BASE_URL"] = anthropic_base_url
            print(f"âœ… Using custom Claude API endpoint: {anthropic_base_url}")

    async def analyze_single_topic(
        self,
        topic: Dict,
        research: Dict
    ) -> Dict:
        """
        Analyze a single trending topic using Claude Agent SDK

        Args:
            topic: Trending topic dictionary
            research: Research findings dictionary

        Returns:
            Product concept dictionary
        """
        keyword = topic["keyword"]
        rank = topic["rank"]
        heat_value = topic["heat_value"]

        # Construct prompt for Claude
        prompt = f"""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„äº§å“è®¾è®¡å¸ˆå’Œå¸‚åœºåˆ†æå¸ˆã€‚è¯·æ ¹æ®ä»¥ä¸‹å¾®åšçƒ­æœè¯é¢˜ï¼Œç”Ÿæˆåˆ›æ„äº§å“æ¦‚å¿µã€‚

**çƒ­æœè¯é¢˜**ï¼š{keyword}
**æ’å**ï¼šç¬¬{rank}å
**çƒ­åº¦å€¼**ï¼š{heat_value:,}

**èƒŒæ™¯ç ”ç©¶**ï¼š
ç¤¾äº¤åª’ä½“è®¨è®ºï¼š
{research['social_media']}

æ–°é—»èƒŒæ™¯ï¼š
{research['news_background']}

ç”¨æˆ·æ´å¯Ÿï¼š
{research['user_insights']}

å¸‚åœºæ½œåŠ›ï¼š
{research['market_potential']}

---

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œè®¾è®¡1ä¸ªåˆ›æ„å°å•†å“ï¼Œå¹¶æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¿”å›JSONï¼ˆä»…è¿”å›JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ï¼‰ï¼š

{{
  "product_name": "äº§å“åç§°ï¼ˆç®€çŸ­ã€æœ‰å¸å¼•åŠ›ï¼‰",
  "market_category": "å¸‚åœºèµ›é“ï¼ˆå¦‚ï¼šæ–‡åˆ›ã€å®¶å±…ã€ç§‘æŠ€é…ä»¶ã€æ—¶å°šé¥°å“ç­‰ï¼‰",
  "target_audience": "ç›®æ ‡äººç¾¤ï¼ˆå…·ä½“æè¿°å¹´é¾„ã€å…´è¶£ã€æ”¶å…¥æ°´å¹³ç­‰ï¼‰",
  "description": "è¯¦ç»†äº§å“æè¿°ï¼ˆå¦‚ä½•ä¸çƒ­æœè¯é¢˜ç»“åˆï¼Œè§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæœ‰ä»€ä¹ˆç‰¹è‰²ï¼‰",
  "manufacturing_details": "ç”Ÿäº§ç‰¹ç‚¹ï¼ˆç”Ÿäº§æ–¹å¼ã€ææ–™ã€èµ·è®¢é‡ã€æˆæœ¬ç»“æ„ç­‰ï¼‰",
  "score_breakdown": {{
    "development_potential": <0-40åˆ†>,
    "interest_level": <0-20åˆ†>,
    "life_utility": <0-20åˆ†>,
    "production_ease": <0-20åˆ†>
  }},
  "total_score": <æ€»åˆ†0-100>,
  "score_justification": "è¯„åˆ†ç†ç”±ï¼ˆç®€è¦è¯´æ˜å„ç»´åº¦è¯„åˆ†ä¾æ®ï¼‰"
}}

**è¯„åˆ†æ ‡å‡†**ï¼š
1. å¯å‘å±•åº¦ (40åˆ†)ï¼šå¸‚åœºè§„æ¨¡15åˆ† + æŠ€æœ¯å¯è¡Œæ€§10åˆ† + è¶‹åŠ¿æŒä¹…æ€§10åˆ† + ç«äº‰æ ¼å±€5åˆ†
2. æœ‰è¶£åº¦ (20åˆ†)ï¼šåˆ›æ„ç‹¬ç‰¹æ€§10åˆ† + æƒ…æ„Ÿå¸å¼•åŠ›5åˆ† + ä¼ æ’­æ½œåŠ›5åˆ†
3. ç”Ÿæ´»æœ‰ç”¨åº¦ (20åˆ†)ï¼šæ—¥å¸¸æ•´åˆåº¦10åˆ† + è§£å†³é—®é¢˜èƒ½åŠ›5åˆ† + å—ä¼—è§„æ¨¡5åˆ†
4. ç”Ÿäº§å®¹æ˜“åº¦ (20åˆ†)ï¼šåˆ¶é€ å¤æ‚åº¦10åˆ† + ææ–™å¯å¾—æ€§5åˆ† + å°æ‰¹é‡æˆæœ¬5åˆ†
"""

        try:
            # Use Claude Agent SDK to generate product concept
            response_text = ""
            async for message in query(prompt=prompt):
                if hasattr(message, 'content'):
                    response_text += str(message.content)
                else:
                    response_text += str(message)

            # Parse JSON response
            # Try to extract JSON from response
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1

            if json_start >= 0 and json_end > json_start:
                json_str = response_text[json_start:json_end]
                concept = json.loads(json_str)

                # Add topic information
                concept["keyword"] = keyword
                concept["rank"] = rank
                concept["heat_value"] = heat_value
                concept["tag"] = topic.get("tag", "")
                concept["category"] = topic.get("category", "")

                # Add research summary
                concept["research_summary"] = research

                # Validate concept
                if validate_product_concept(concept):
                    # Calculate tier
                    tier_name, tier_badge, tier_class = calculate_score_tier(
                        concept["total_score"]
                    )
                    concept["tier_name"] = tier_name
                    concept["tier_badge"] = tier_badge
                    concept["tier_class"] = tier_class

                    return concept
                else:
                    print(f"âš ï¸  Invalid product concept for '{keyword}'")
                    return self._create_fallback_concept(topic, research)

            else:
                print(f"âš ï¸  Failed to parse JSON for '{keyword}'")
                return self._create_fallback_concept(topic, research)

        except Exception as e:
            print(f"âŒ Error analyzing topic '{keyword}': {e}")
            return self._create_fallback_concept(topic, research)

    def _create_fallback_concept(self, topic: Dict, research: Dict) -> Dict:
        """Create a basic fallback concept when AI analysis fails"""
        return {
            "keyword": topic["keyword"],
            "rank": topic["rank"],
            "heat_value": topic["heat_value"],
            "tag": topic.get("tag", ""),
            "category": topic.get("category", ""),
            "product_name": f"{topic['keyword']}ä¸»é¢˜å•†å“",
            "market_category": "æ–‡åˆ›äº§å“",
            "target_audience": "18-35å²å¹´è½»äººç¾¤",
            "description": f"åŸºäºçƒ­æœè¯é¢˜'{topic['keyword']}'çš„åˆ›æ„äº§å“",
            "manufacturing_details": "å°æ‰¹é‡ç”Ÿäº§ï¼Œå¾…è¿›ä¸€æ­¥åˆ†æ",
            "score_breakdown": {
                "development_potential": 20,
                "interest_level": 10,
                "life_utility": 10,
                "production_ease": 10
            },
            "total_score": 50,
            "score_justification": "âš ï¸ AIåˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤è¯„åˆ†",
            "research_summary": research,
            "tier_name": "å…¶ä»–",
            "tier_badge": "ğŸ“‹ å…¶ä»–",
            "tier_class": "other"
        }

    async def analyze_trends(self, limit: int = 10) -> Dict:
        """
        Main analysis workflow

        Args:
            limit: Number of trends to analyze

        Returns:
            Complete analysis results dictionary
        """
        print(f"ğŸš€ Starting Weibo Trends Analysis...")
        print(f"ğŸ“… {format_display_timestamp()}\n")

        # Step 1: Fetch trending topics
        print("ğŸ“Š Step 1: Fetching Weibo trending topics...")
        topics = self.weibo_client.fetch_trending_topics(limit=limit)

        if not topics:
            print("âŒ No topics fetched. Exiting.")
            return {"error": "No topics available"}

        print(f"âœ… Fetched {len(topics)} trending topics\n")

        # Step 2: Research and analyze each topic
        print("ğŸ” Step 2: Researching and analyzing topics...")
        product_concepts = []

        for idx, topic in enumerate(topics, 1):
            keyword = topic["keyword"]
            print(f"\n[{idx}/{len(topics)}] Analyzing: {keyword}")

            # Conduct web research
            print(f"  ğŸ” Researching background...")
            research = self.search_client.research_topic(keyword)

            # Analyze with Claude
            print(f"  ğŸ¤– Generating product concept with AI...")
            concept = await self.analyze_single_topic(topic, research)

            product_concepts.append(concept)
            print(f"  âœ… {concept['product_name']} - Score: {concept['total_score']}/100 ({concept['tier_badge']})")

        # Step 3: Sort and categorize
        print(f"\nğŸ“Š Step 3: Organizing results...")
        product_concepts.sort(key=lambda x: x["total_score"], reverse=True)

        # Categorize by tier
        excellent = [p for p in product_concepts if p["total_score"] >= 80]
        good = [p for p in product_concepts if 60 <= p["total_score"] < 80]
        other = [p for p in product_concepts if p["total_score"] < 60]

        # Calculate statistics
        avg_score = sum(p["total_score"] for p in product_concepts) / len(product_concepts) if product_concepts else 0

        results = {
            "metadata": {
                "generated_at": format_display_timestamp(),
                "total_analyzed": len(product_concepts),
                "average_score": round(avg_score, 1),
                "excellent_count": len(excellent),
                "good_count": len(good),
                "other_count": len(other)
            },
            "products": {
                "excellent": excellent,
                "good": good,
                "other": other
            },
            "all_products": product_concepts
        }

        print(f"\nâœ… Analysis complete!")
        print(f"  ğŸ† Excellent (â‰¥80): {len(excellent)}")
        print(f"  â­ Good (60-79): {len(good)}")
        print(f"  ğŸ“‹ Other (<60): {len(other)}")
        print(f"  ğŸ“Š Average Score: {avg_score:.1f}/100")

        return results

    def generate_html_report(self, results: Dict, output_dir: str = "reports") -> str:
        """
        Generate HTML report from analysis results

        Args:
            results: Analysis results dictionary
            output_dir: Output directory

        Returns:
            Path to generated HTML file
        """
        print(f"\nğŸ“ Generating HTML report...")

        # Load template
        template_dir = os.path.join(os.path.dirname(__file__), "templates")
        env = Environment(loader=FileSystemLoader(template_dir))
        template = env.get_template("dashboard_template.html")

        # Render template
        html_content = template.render(
            metadata=results["metadata"],
            excellent_products=results["products"]["excellent"],
            good_products=results["products"]["good"],
            other_products=results["products"]["other"]
        )

        # Save HTML file
        os.makedirs(output_dir, exist_ok=True)
        filename = f"weibo-trends-analysis-{format_timestamp()}.html"
        filepath = os.path.join(output_dir, filename)

        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(html_content)

        print(f"âœ… HTML report saved: {filepath}")
        return filepath


async def main():
    """Main entry point"""
    # Load configuration from environment variables
    tianapi_key = os.getenv("TIANAPI_KEY")
    search_api_key = os.getenv("SEARCH_API_KEY")
    anthropic_api_key = os.getenv("ANTHROPIC_API_KEY")
    anthropic_base_url = os.getenv("ANTHROPIC_BASE_URL")  # Optional: for third-party APIs
    search_engine = os.getenv("SEARCH_ENGINE", "serpapi")  # serpapi or google
    analysis_limit = int(os.getenv("ANALYSIS_LIMIT", "10"))

    # Validate required environment variables
    if not all([tianapi_key, search_api_key, anthropic_api_key]):
        print("âŒ Error: Missing required environment variables")
        print("Required: TIANAPI_KEY, SEARCH_API_KEY, ANTHROPIC_API_KEY")
        sys.exit(1)

    # Initialize analyzer
    analyzer = WeiboTrendsAnalyzer(
        tianapi_key=tianapi_key,
        search_api_key=search_api_key,
        anthropic_api_key=anthropic_api_key,
        search_engine=search_engine,
        anthropic_base_url=anthropic_base_url
    )

    # Run analysis
    results = await analyzer.analyze_trends(limit=analysis_limit)

    if "error" in results:
        print(f"âŒ Analysis failed: {results['error']}")
        sys.exit(1)

    # Save JSON data (optional)
    json_filename = f"weibo-trends-data-{format_timestamp()}.json"
    json_path = save_json_data(results, "reports", json_filename)
    print(f"âœ… JSON data saved: {json_path}")

    # Generate HTML report
    html_path = analyzer.generate_html_report(results)

    print(f"\nğŸ‰ All done! Check the reports in the 'reports/' directory.")
    print(f"ğŸ“„ HTML Report: {html_path}")
    print(f"ğŸ“Š JSON Data: {json_path}")


if __name__ == "__main__":
    asyncio.run(main())
